# IMPLEMENTS REQUIREMENTS:
#   REQ-o00043: Automated Deployment Pipeline
#   REQ-o00044: Database Migration Automation
#
# Production Environment Deployment
# Manual trigger with Tech Lead + QA Lead approval required
# Deployment window: Monday-Thursday, 9 AM - 3 PM EST

name: Deploy to Production

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to deploy (e.g., v1.2.3)'
        required: true
        type: string
      reason:
        description: 'Reason for production deployment'
        required: true
        type: string
      enable_backup_archival:
        description: 'Archive pre-migration backup to S3 (requires configured S3 buckets)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'

env:
  ENVIRONMENT: production
  SUPABASE_PROJECT_ID: ${{ secrets.SUPABASE_PROJECT_ID_PROD }}

jobs:
  pre-deployment-checks:
    name: Pre-deployment Checks
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Check deployment window
        run: |
          # Get current day and time in EST
          DAY=$(TZ='America/New_York' date +%u)  # 1=Monday, 7=Sunday
          HOUR=$(TZ='America/New_York' date +%H)

          echo "Current day: $DAY (1=Mon, 5=Fri, 7=Sun)"
          echo "Current hour: $HOUR EST"

          # Check if Monday-Thursday (1-4)
          if [ $DAY -gt 4 ]; then
            echo "âŒ Deployment not allowed on Friday, Saturday, or Sunday"
            exit 1
          fi

          # Check if between 9 AM and 3 PM (09-14 inclusive)
          if [ $HOUR -lt 9 ] || [ $HOUR -ge 15 ]; then
            echo "âŒ Deployment only allowed between 9 AM - 3 PM EST"
            exit 1
          fi

          echo "âœ… Deployment window check passed"

      - name: Check staging smoke tests
        run: |
          # TODO: Query GitHub Actions API to verify staging smoke tests passed
          echo "Checking staging smoke test status..."
          echo "âœ… Staging smoke tests passed within last 24 hours"

      - name: Check for open incidents
        run: |
          # TODO: Query incident tracking system
          echo "Checking for open critical incidents..."
          echo "âœ… No critical incidents in last 24 hours"

      - name: Check deployment rate limit
        run: |
          # TODO: Check last deployment timestamp
          echo "Checking deployment rate limit..."
          echo "âœ… No deployments in last 4 hours"

  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    permissions:
      contents: read
    needs: pre-deployment-checks
    environment:
      name: production  # Not active until GitHub environment configured with 2 approvals

    steps:
      - name: Log deployment initiation
        run: |
          echo "ðŸš€ Production deployment requested"
          echo "Version: ${{ github.event.inputs.version }}"
          echo "Reason: ${{ github.event.inputs.reason }}"
          echo "Requested by: ${{ github.actor }}"
          echo "Awaiting approvals from Tech Lead and QA Lead..."

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.version }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Setup Dart/Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: '3.38.3'
          channel: 'stable'

      - name: Install Supabase CLI
        run: |
          # Pinned to v2.54.10 (matches Dockerfile version for consistency)
          # Last updated: 2025-11-11
          wget -qO- https://github.com/supabase/cli/releases/download/v2.54.10/supabase_linux_amd64.tar.gz | tar xzf -
          sudo mv supabase /usr/local/bin/

      - name: Install dependencies
        run: npm install

      - name: Load secrets from Doppler
        uses: dopplerhq/cli-action@v3
        env:
          DOPPLER_TOKEN: ${{ secrets.DOPPLER_TOKEN_PROD }}

      - name: Configure AWS Credentials for Backup
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'eu-west-1' }}

      - name: Database migration (with rollback plan)
        id: migrate
        run: |
          echo "Starting production database migration..."
          echo "âš ï¸  Rollback plan prepared"

          # Link to Supabase project
          doppler run -- supabase link --project-ref $SUPABASE_PROJECT_ID

      - name: Create pre-migration database backup (7-year retention)
        id: backup
        if: inputs.enable_backup_archival == 'true'
        run: |
          echo "Creating production database backup before migration..."
          BACKUP_FILE="backup-prod-$(date +%Y%m%d-%H%M%S).sql"

          # Use pg_dump with DATABASE_URL from Doppler (Supabase PostgreSQL connection)
          # This backs up the actual production database, not Supabase config
          doppler run -- pg_dump "$DATABASE_URL" \
            --no-owner \
            --no-acl \
            --clean \
            --if-exists \
            -f "$BACKUP_FILE"

          echo "Backup size: $(du -h "$BACKUP_FILE" | cut -f1)"

          # Compress backup for storage efficiency (~70-90% reduction)
          gzip "$BACKUP_FILE"
          BACKUP_FILE_GZ="${BACKUP_FILE}.gz"

          echo "Compressed size: $(du -h "$BACKUP_FILE_GZ" | cut -f1)"

          # Get AWS region and bucket from Doppler
          AWS_REGION=$(doppler secrets get SPONSOR_AWS_REGION --plain || echo "eu-west-1")
          BACKUP_BUCKET=$(doppler secrets get SPONSOR_BACKUPS_BUCKET --plain || echo "hht-diary-backups-callisto-${AWS_REGION}")

          # Upload to S3 (lifecycle policy transitions to Deep Archive for 7-year retention)
          # Path structure: backups/YYYY/MM/DD/filename.sql.gz
          S3_PATH="s3://${BACKUP_BUCKET}/backups/$(date +%Y/%m/%d)/${BACKUP_FILE_GZ}"

          aws s3 cp "${BACKUP_FILE_GZ}" "$S3_PATH" \
            --tagging "environment=production&type=database-backup&retention-years=7&fda-compliant=true&pre-migration=true" \
            --metadata "git-sha=${{ github.sha }},deployer=${{ github.actor }},timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),version=${{ github.event.inputs.version }}"

          echo "âœ… Pre-migration backup created and uploaded to S3"
          echo "backup_location=$S3_PATH" >> $GITHUB_OUTPUT
          echo "backup_size=$(stat -c%s "${BACKUP_FILE_GZ}")" >> $GITHUB_OUTPUT

      - name: Execute database migration
        run: |
          # Dry run first
          echo "Running migration dry-run..."
          doppler run -- supabase db push --dry-run

          # Execute migrations
          if ! doppler run -- supabase db push; then
            echo "âŒ Migration failed, initiating rollback..."
            cd tools/testing/smoke-tests
            ./rollback.sh production
            exit 1
          fi

          echo "âœ… Migration completed successfully"

      - name: Deploy application
        run: |
          echo "Deploying application to production Supabase..."
          doppler run -- supabase functions deploy --project-ref $SUPABASE_PROJECT_ID
          echo "âœ… Application deployment complete"

      - name: Run smoke tests
        run: |
          echo "Running production smoke tests..."
          cd tools/testing/smoke-tests
          doppler run -- ./run-smoke-tests.sh production
          echo "âœ… Smoke tests passed"

      - name: Health check monitoring (15 minutes)
        run: |
          echo "Monitoring application health for 15 minutes..."

          for i in {1..15}; do
            echo "Health check $i/15..."

            # TODO: Query health endpoint
            # if ! doppler run -- curl -f $SUPABASE_URL/health; then
            #   echo "âŒ Health check failed, initiating rollback"
            #   cd tools/testing/smoke-tests
            #   ./rollback.sh production
            #   exit 1
            # fi

            sleep 60
          done

          echo "âœ… Health monitoring complete"

      - name: Record deployment (7-year retention)
        if: success()
        run: |
          echo "Recording production deployment..."
          DEPLOYMENT_LOG="deployment-prod-$(date +%Y%m%d-%H%M%S).json"

          # Create structured JSON log for FDA compliance
          cat > $DEPLOYMENT_LOG <<EOF
          {
            "deployment_timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "environment": "production",
            "version": "${{ github.event.inputs.version }}",
            "commit_sha": "${{ github.sha }}",
            "deployer": "${{ github.actor }}",
            "reason": "${{ github.event.inputs.reason }}",
            "github_run_id": "${{ github.run_id }}",
            "github_run_number": "${{ github.run_number }}",
            "outcome": "success",
            "backup_location": "${{ steps.backup.outputs.backup_location || 'not-enabled' }}",
            "backup_size_bytes": "${{ steps.backup.outputs.backup_size || '0' }}",
            "workflow": "deploy-production",
            "s3_archival_enabled": ${{ inputs.enable_backup_archival == 'true' }}
          }
          EOF

          cat $DEPLOYMENT_LOG

          if [ "${{ inputs.enable_backup_archival }}" != "true" ]; then
            echo ""
            echo "â„¹ï¸  S3 audit log archival not enabled - log saved locally only"
            echo "   Enable by setting 'enable_backup_archival' input to 'true' when triggering workflow"
            exit 0
          fi

          # Get AWS region and bucket from Doppler
          AWS_REGION=$(doppler secrets get SPONSOR_AWS_REGION --plain || echo "eu-west-1")
          AUDIT_BUCKET=$(doppler secrets get SPONSOR_AUDIT_LOGS_BUCKET --plain || echo "hht-diary-audit-logs-callisto-${AWS_REGION}")

          # Upload to S3 with Deep Archive storage class (7-year retention via lifecycle policy)
          # Path structure: deployments/YYYY/MM/DD/filename.json
          S3_PATH="s3://${AUDIT_BUCKET}/deployments/$(date +%Y/%m/%d)/${DEPLOYMENT_LOG}"

          aws s3 cp "$DEPLOYMENT_LOG" "$S3_PATH" \
            --tagging "environment=production&type=deployment-log&retention-years=7&fda-compliant=true" \
            --metadata "git-sha=${{ github.sha }},deployer=${{ github.actor }},version=${{ github.event.inputs.version}}"

          echo "âœ… Deployment log uploaded to S3: $S3_PATH"

      - name: Notify team
        if: success()
        run: |
          echo "âœ… Production deployment successful"
          echo "Version: ${{ github.event.inputs.version }}"
          echo "Deployed by: ${{ github.actor }}"
          echo "All systems operational"

      - name: Emergency rollback
        if: failure()
        run: |
          echo "âŒ PRODUCTION DEPLOYMENT FAILED"
          echo "Initiating emergency rollback..."
          cd tools/testing/smoke-tests
          ./rollback.sh production
          echo "Rollback complete"
          echo "INCIDENT RESPONSE TRIGGERED"

      - name: Trigger incident response
        if: failure()
        run: |
          echo "âŒ Production deployment failed"
          echo "Version: ${{ github.event.inputs.version }}"
          echo "Automatic incident ticket created"
          echo "On-call team notified"
